# 第三周

## Thinking1 什么是监督学习，无监督学习，半监督学习

**1、有监督学习：**

①从给定的训练数据集中学习出一个函数（模型参数），当新的数据到来时，可以根据这个函数预测结果。

②监督学习的训练集要求包括输入输出，也可以说是特征和目标。训练集中的目标是由人标注的。

③监督学习就是最常见的分类（注意和聚类区分）问题，通过已有的训练样本（即已知数据及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优表示某个评价准则下是最佳的），再利用这个模型将所有的输入映射为相应的输出，对输出进行简单的判断从而实现分类的目的。也就具有了对未知数据分类的能力。

④监督学习的目标往往是让计算机去学习我们已经创建好的分类系统（模型）

**2、无监督学习：**

①输入数据没有被标记，也没有确定的结果。

②样本数据类别未知，需要根据样本间的相似性对样本集进行分类（聚类，clustering）试图使类内差距最小化，类间差距最大化。

③通俗点将就是实际应用中，不少情况下无法预先知道样本的标签，也就是说没有训练样本对应的类别，因而只能从原先没有样本标签的样本集开始学习分类器设计。

④非监督学习目标不是告诉计算机怎么做，而是让它（计算机）自己去学习怎样做事情。

**3、半监督学习：**

半监督学习是监督学习和非监督学习的混合体，训练数据包括标注数据和非标注数据。根据学习目标分为两类：一类是监督学习任务，得到输入—输出的映射函数，利用未标注数据进行函数的优化；另一类是非监督学习任务，得到聚类的结果，利用标注数据提高效果。

## Thinking2 K-means中的k值如何选取

**手肘法**

手肘法的核心指标是SSE(sum of the squared errors，即误差平方和)。

1、手肘法的核心思想是：随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。并且，当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而这个肘部对应的k值就是数据的真实聚类数。

2、具体操作：找到聚类的数量K与各个点到簇中心的距离的平方的和所构成的函数的极值点，其极值点所对应的K值即为所求。



## Thinking3 随机森林采用了bagging集成学习，bagging指的是什么

bagging随机有放回地抽取训练集，训练多个弱分类器。然后通过取平均，或者投票(voting)的方式决定最终的分类结果。


Bagging 主要关注降低方差，是要降低过拟合，而不会降低偏差，因此最好不要用高偏差的模型。

在不剪枝决策树，神经网络等易受样本扰动的学习器上效用更为明显。例如当基学习器是决策树时，Bagging 是并行的生成多个决策树，此时可以不做剪枝，这样每个都是强学习器，就会有过拟合的问题，但是多个学习器组合在一起，可以降低过拟合。

## Thinking4 表征学习和半监督学习的区别是什么

1、表征学习： 
机器学习算法的成功与否不仅仅取决于算法本身，也取决于数据的表示
表征学习（Representation Learning），也称为特征学习(feature learning)目的是对复杂的原始数据化繁为简，把原始数据的无效信息剔除，把有效信息更有效地进行提炼，形成特征 

2、半监督学习： 
通常半监督学习的任务与监督学习一致，即任务中包含有明确的目标（如分类），采用的数据既包括有标签的数据，也包括无标签的数据
作用：只有少量的数据有Label，利用没有标签的数据来学习整个数据的潜在分布
