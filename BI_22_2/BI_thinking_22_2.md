# 第二十二周作业——周日名企课

### Thinking1  机器学习中的监督学习、非监督学习、强化学习有何区别

**1. 强化学习与监督学习的区别**

①有监督学习的训练样本是有标签的，强化学习的训练是没有标签的，它是通过环境给出的奖惩来学习

②有监督学习的学习过程是静态的，强化学习的学习过程是动态的。有监督学习是给什么样本就学什么，而强化学习是要和环境进行交互，再通过环境给出的奖惩来学习

③监督学习希望能将学习的结果运用到未知的数据，要求结果可推广、可泛化；强化学习的结果却可以用在训练的环境中。强化学习与监督学习的区别

**2. 强化学习与非监督学习的区别在于：**

①非监督学习旨在发现数据之间隐含的结构，一般用于聚类等任务。
②强化学习有着明确的数值目标奖励。

**3.监督学习与非监督学习**

①监督学习：利用训练数据集学习一个模型，再用模型对测试样本集进行预测

②非监督学习：直接对数据进行建模。没有给定事先标记过的训练范例，所用的数据没有属性或标签这一概念。事先不知道输入数据对应的输出结果是什么，自动对输入的资料进行分类或分群，以寻找数据的模型和规律



### Thinking2  什么是策略网络，价值网络，有何区别

1. 策略网络

   通过建立一个神经网络模型，观察环境状态预测出目前应该执行的策略并执行，获取可以获得最大的奖励是多少。和普通的监督学习不同，策略网络不是通过feature预测label，而是根据对观察的环境状态进入模型，得到action，执行这个action后得到reward，通过reward的加权衰减并叠加后计算梯度，通过梯度优化网络参数。

2. 价值网络

   通过计算目前状态s的累积分数的期望，价值网络给游戏中的状态赋予一个分数（数值），每个状态都经历了整个数值网络； 奖励更多的状态，会在数值网络中的数值Value更大。

3. 区别

   ① 价值网络的输出，一个可能获胜的数值；而策略网络的输出，是一个落子的概率分布；

   ②策略网络能学到一些随机策略，而价值网络通常是学不到随机策略。



### Thinking3  请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的

**原理**：

蒙特卡罗方法通过抓住事物运动的几何数量和几何特征，利用数学方法来加以模拟，即进行一种数字模拟实验，以一个概率模型为基础，按照这个模型所描绘的过程，通过模拟实验的结果，作为问题的近似解。可以把蒙特卡罗解题归结为三个主要步骤：构造或描述概率过程，实现从已知概率分布抽样，建立各种估计量

**步骤：**

①选择（Select），从根节点开始，按一定策略，搜索到叶子节点；

②step2：扩展（Expansion），对叶子节点扩展一个或多个合法的子节点

③step3：模拟（Simluation），对子节点采用随机的方式(这也是为什么称之为蒙特卡洛的原因)模拟若干次实验。模拟到最终状态时即可得到当前模拟所得的分数；

④step4：回传（Backpropagation），根据子节点若干次模拟的得分，更新当前子节点的模拟次数与得分值。同时将模拟次数与得分值回传到其所有祖先节点并更新祖先节点。



### Thinking4  假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑

1. 使用强化学习与自适应在线学习，通过持续机器学习和模型优化建立决策引擎，对海量用户行为进行实时分析，帮用户快速发现喜欢的短视频并推荐。
2. 考虑用户点赞、收藏、转发行为，将行为作为奖励，反馈给强化学习的模型，通过强化学习学习出用户的喜好。

### Thinking5  在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路

1. 强化学习Agent：车

2. Action包括加速、减速、转向

3. State，Agent从环境获取的信息

4. 奖励，每次执行动作后，到达目的地如果路程更短，则执行对动作的奖励；若安全抵达就给更多奖励；反之则为惩罚

5. 每一步行动都给与反馈，不断调整训练对象的行为。