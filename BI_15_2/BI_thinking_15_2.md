# 第十五周作业——周日名企课

## Thinking1  逻辑回归的假设条件是怎样的？

 逻辑回归的假设条件主要有两个：

1. 假设数据服从伯努利分布(非正即反，概率和为1)。
2. 假设模型的输出值是样本为正例的概率。

$$
h_\theta(x;\theta) = \frac{1}{1+e^{-\theta^{T}x}}
$$



## Thinking2  逻辑回归的损失函数是怎样的？

1. *y*=0时，

$$
Cost(h_\theta(x), y) = −log(1−h_\theta(x))
$$
2. *y*=1时，

$$
Cost(h_\theta(x), y) = −log(h_\theta(x))
$$


逻辑回归的损失函数是它的极大似然函数:
$$
L\theta = logL(\theta) = -\frac{1}{m}\sum_{i=1}^{m}(y^{i}log(h_{\theta}(x^i)) + (1-y^{i})log(1-h_{\theta}(x^i))
$$
$y^(i)$指样本$i$的真实label值，而$h_{\theta}(x^i)$指预测为该label的概率；由于概率取对数后为负数，且概率值越高，对数值越高；且我们的目标是希望预测正确的损失函数最小，所以损失函数会在最后使用 - 取反。



## Thinking3  逻辑回归如何进行分类？

1. 划定一个阈值，正类的概率大于这个阈值的则预测为正类，小于这个阈值则预测为负累。

2. 阈值具体如何调整根据实际情况选择。一般会取阈值为0.5。



## Thinking4  为什么在训练中需要将高度相关的特征去掉？

1. 高度相关特征导致线性回归估计式变得不确定或不精确；
2. 高度相关特征导致线性回归估计式方差变得很大，标准误差增大；
3. 高度相关特征导致高度相关的特征太多的话，可能使估计的回归系数符号相反，得出错误的结论；
4. 高度相关特征导致而且会削弱特征变量的特征重要性。

5. 去掉后会让模型的可解释性更好；

6. 去掉后，特征减少，会提高训练的速度。